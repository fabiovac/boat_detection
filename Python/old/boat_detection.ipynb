{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Svq98spyi_Yx"
   },
   "source": [
    "#Imports and Drive mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "SNxPbN6bjCS_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "#??\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcAfgnYy1tD7",
    "outputId": "2a8d9a37-faf9-4853-e35a-5e059c837531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DN4NDRpv_RYU"
   },
   "source": [
    "#Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "tZsxbMkoZB0j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocotools in /Users/fabio/opt/anaconda3/lib/python3.8/site-packages (2.0.2)\n",
      "Requirement already satisfied: cython>=0.27.3 in /Users/fabio/opt/anaconda3/lib/python3.8/site-packages (from pycocotools) (0.29.21)\n",
      "Requirement already satisfied: setuptools>=18.0 in /Users/fabio/opt/anaconda3/lib/python3.8/site-packages (from pycocotools) (50.3.1.post20201107)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /Users/fabio/opt/anaconda3/lib/python3.8/site-packages (from pycocotools) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /Users/fabio/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/fabio/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /Users/fabio/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools) (1.19.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/fabio/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools) (7.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/fabio/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/fabio/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/fabio/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n",
      "Requirement already satisfied: six in /Users/fabio/opt/anaconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/fabio/Documents/Università/Computer Vision/Progetto/BoatDetection/modules/')\n",
    "\n",
    "!{sys.executable} -m pip install pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab5U10atjckU"
   },
   "source": [
    "#Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "yoODM36jhzYh"
   },
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "class BoatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.train = True\n",
    "        self.transforms = self.get_transform()\n",
    "\n",
    "        # Load the annotations from the COCO json file\n",
    "        self.annotations_json = json.load(open(f'{self.dataset_path}kaggle_annotations.json'))\n",
    "        \n",
    "        #self.annotations = list(annotations1.values())  # don't need the dict keys\n",
    "\n",
    "        self.images_name = []\n",
    "        for filename in glob.glob(f'{self.dataset_path}*.jpg'):\n",
    "            self.images_name.append(filename.split('/')[-1])\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Initialize the output target\n",
    "        target = {}\n",
    "\n",
    "        # Get details of the image with index idx\n",
    "        image_name = self.images_name[idx] # Image name\n",
    "        image_path = os.path.join(self.dataset_path, image_name) # Image path\n",
    "        image = Image.open(image_path).convert(\"RGB\") # Image file\n",
    "        image_json = list(filter(lambda image: image['file_name'] == image_name, self.annotations_json['images'])) # Image json from the annotations\n",
    "\n",
    "        ###print(f'Image name: {image_name}')\n",
    "        # Check if the image is annotated\n",
    "        if len(image_json) > 0:\n",
    "\n",
    "            image_json = image_json[0]\n",
    "\n",
    "            # Get the annotations relevant to the image\n",
    "            annotations = list(filter(lambda annotation: annotation['image_id'] == image_json['id'], self.annotations_json['annotations']))\n",
    "            annotations_num = len(annotations)\n",
    "            ###print(f'    Annotation: {annotations}')\n",
    "    \n",
    "            # Calculate bounding boxes (converting from (x1, y1, width, height) to (x1, y1, x2, y2))\n",
    "            boxes = []\n",
    "            for annotation in annotations:\n",
    "                box = annotation['bbox']\n",
    "                boxes.append([box[0], box[1], box[0]+box[2], box[1]+box[3]])\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "\n",
    "            # Put the labels to 1 (the only class we have). 0 is the background\n",
    "            labels = torch.ones((annotations_num,), dtype=torch.int64)\n",
    "\n",
    "            # Put the image_id equal to the index we provide to the Dataset\n",
    "            image_id = torch.tensor([idx])\n",
    "\n",
    "            # Calculate the area of the bounding box\n",
    "            area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "\n",
    "            # No crowd in our dataset\n",
    "            iscrowd = torch.zeros((annotations_num,), dtype=torch.int64)\n",
    "        \n",
    "            target[\"boxes\"] = boxes\n",
    "            target[\"labels\"] = labels\n",
    "            target[\"image_id\"] = image_id\n",
    "            target[\"area\"] = area\n",
    "            target[\"iscrowd\"] = iscrowd\n",
    "        else:\n",
    "            target = {\n",
    "                'boxes': torch.zeros((0, 4), dtype=torch.float32),\n",
    "                'labels': torch.zeros(0, dtype=torch.int64),\n",
    "                'image_id': torch.tensor([idx]),\n",
    "                'area': torch.zeros(0, dtype=torch.float32),\n",
    "                'iscrowd': torch.zeros((0,), dtype=torch.int64)\n",
    "            }\n",
    "            ###print('    No annotation')\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_name)\n",
    "\n",
    "    def get_transform(self):\n",
    "        transforms = []\n",
    "\n",
    "        # Converts the PIL image into a PyTorch Tensor\n",
    "        transforms.append(T.ToTensor())\n",
    "\n",
    "        if self.train == True:\n",
    "            # Flip horizontally and randomly during training\n",
    "            transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "\n",
    "        return T.Compose(transforms)\n",
    "\n",
    "    def set_val(self):\n",
    "        self.train = False\n",
    "        self.transforms = self.get_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqGpdKgrgHZq",
    "outputId": "a0ee5dd6-40ce-42ea-9158-63bf455c3ac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size = 1167, Val dataset size = 292\n"
     ]
    }
   ],
   "source": [
    "# Create the entire dataset\n",
    "dataset = BoatDataset('/Users/fabio/Documents/Università/Computer Vision/Progetto/BoatDetection/datasets/kaggle_dataset/')\n",
    "\n",
    "# Calculate the number of train and validation images\n",
    "train_percentage = 0.8\n",
    "train_images_num = round(len(dataset) * 0.8)\n",
    "val_images_num = len(dataset) - train_images_num\n",
    "print(f'Train dataset size = {train_images_num}, Val dataset size = {val_images_num}')\n",
    "\n",
    "# Split the original dataset into the train dataset and the validation dataset\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_images_num, val_images_num], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Set the train mode to False\n",
    "val_dataset.dataset.set_val()\n",
    "\n",
    "# Initialize training and validation DataLoaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2, collate_fn=utils.collate_fn)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgVbWW-w9GcZ"
   },
   "source": [
    "#Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "roIXC47P9Ij5"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "      \n",
    "def build_model(num_classes):\n",
    "    # Load an instance of a pre-trained model (Faster-RCNN)\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # Replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjwoF52-ok5L"
   },
   "source": [
    "#Initialize model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Naj7vJnnoor6"
   },
   "outputs": [],
   "source": [
    "# Choose which device to use\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Out model has two classes (background = 0 and boat = 1)\n",
    "num_classes = 2\n",
    "\n",
    "# Get the model using the helper function\n",
    "model = build_model(num_classes)\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Construct the optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Learning rate scheduler which decreases the learning rate by 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MH4VarltaTLj"
   },
   "source": [
    "#Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONUNBUFFERED=FALSE\n"
     ]
    }
   ],
   "source": [
    "%env PYTHONUNBUFFERED=FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdSarBmAXk2g",
    "outputId": "1ead13f7-5cce-41b7-c920-4599a38d207c"
   },
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "num_epochs = 10\n",
    " \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, train_dataloader, device, epoch, print_freq=1)\n",
    "    \n",
    "    # Update the learning rate\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Evaluate on the val dataset\n",
    "    evaluate(model, val_dataloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "boat_detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
